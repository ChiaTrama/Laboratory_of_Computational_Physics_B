{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8788f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Hide GPU from visible devices\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '12'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS']='1'\n",
    "#os.environ['CUDA_VISIBLE_DEVICES']='-1'\n",
    "#tf.config.threading.set_inter_op_parallelism_threads(3) \n",
    "#tf.config.threading.set_intra_op_parallelism_threads(3)\n",
    "#tf.config.set_soft_device_placement(enabled = True)\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' #to hide tensorflow warnings\n",
    "\n",
    "tf.config.experimental.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c333a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#input data function\n",
    "def load_input_data(input_file):\n",
    "    with open(input_file, 'rb') as file:\n",
    "        dataset = pickle.load(file)\n",
    "        # For our problem we have positions and energy arrays,\n",
    "        # which are respectively data and labels in common Machine Learning language\n",
    "        positions_array = np.array([mol['positions'] for mol in dataset])\n",
    "        energies_array = np.array([mol['energy'] for mol in dataset])\n",
    "\n",
    "    return positions_array, energies_array\n",
    "\n",
    "#model creation\n",
    "def createmodel(input_class, output_class, base_neurons,batch_size, l2par):\n",
    "    model = Sequential([\n",
    "        Dense(base_neurons, activation='relu', kernel_regularizer=l2(l2par)), #input_shape=(input_class,),\n",
    "        Dense(base_neurons/2., activation='relu', kernel_regularizer=l2(l2par)),\n",
    "        Dense(base_neurons/4., activation='gelu', kernel_regularizer=l2(l2par)),\n",
    "        Dense(base_neurons/8., activation='gelu', kernel_regularizer=l2(l2par)),\n",
    "        Dense(base_neurons/16., activation='relu', kernel_regularizer=l2(l2par)),\n",
    "        Dense(output_class)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "#model compilation\n",
    "def compilemodel(model,opt):\n",
    "    #optimizer training with type of dataset\n",
    "    model.compile(\n",
    "        #rms\n",
    "        loss='mean_squared_error',\n",
    "        #adam with LR and decay\n",
    "        optimizer=opt,\n",
    "        #metrics to monitor\n",
    "        metrics=['mean_squared_error', 'mae']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c77fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define input train and test data directory\n",
    "#input_file = 'saved_dataset/merged_dataset_rand_5a.pickle'\n",
    "input_file = 'saved_dataset/merged_randcart5_dataset_5a.pickle'\n",
    "#Load dataset into data and labels (positions=data, energy=labels)\n",
    "positions_array, energies_array = load_input_data(input_file)\n",
    "\n",
    "# Splitting the dataset into training, validation, and test sets\n",
    "# with a 72-20-8% ratio from initial dataset\n",
    "train_val_data, test_data, train_val_labels, test_labels = train_test_split(\n",
    "    positions_array, energies_array, test_size=0.01, random_state=42)\n",
    "# Further splitting train, into new train and validation samples\n",
    "# Takes 80% and is divided to 10% of that for the validation set\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(\n",
    "    train_val_data, train_val_labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(train_data.shape[0], 'train samples')\n",
    "print(test_data.shape[0], 'test samples')\n",
    "print(val_data.shape[0], 'validation samples')\n",
    "print(train_data.shape[1], 'train shape')\n",
    "#print(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e627bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define input, output dimensions\n",
    "input_class = train_data.shape[1] #dH1,dH2,theta, in Angstrom units\n",
    "output_class = 1 #total system energy in eV\n",
    "n_iterations = 1\n",
    "#define learing rate, epochs and batch_size for optimizer and fitter\n",
    "batch_size = 128\n",
    "epochs = 40000\n",
    "neurons = 256\n",
    "l2par=0.001\n",
    "patience=150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeb37fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base model creation\n",
    "#iterations = n_iterations\n",
    "#increment = (end_neurons- base_neurons) // (iterations-1)\n",
    "#neurons = base_neurons + i*increment\n",
    "model = createmodel(input_class, output_class, neurons, batch_size, l2par)\n",
    "#neurons = neurons*2\n",
    "    \n",
    "#model name for saving purposes\n",
    "ainame = 'Adadelta6_'\n",
    "n_layers = len(model.layers)\n",
    "specs = str(n_layers)+'layers_'+str(neurons)+'neurons_'\n",
    "specs2 = str(epochs)+'epochs_'+str(batch_size)+'batchsize_'+str(l2par)+'l2par'\n",
    "\n",
    "model_name = str(ainame) + str(specs) + str(specs2)\n",
    "\n",
    "\n",
    "fitstart = time.time()\n",
    "# Model compilation with automatic LR by adam and adamax to fine descent\n",
    "# Model training \n",
    "compilemodel(model,keras.optimizers.Adadelta())\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=patience)\n",
    "history=model.fit(\n",
    "    train_data, train_labels, epochs=epochs, batch_size=batch_size, \n",
    "    validation_data=(val_data, val_labels), \n",
    "    callbacks=callback, verbose = 1\n",
    "    )\n",
    "\n",
    "#second part of the training with more relaxed optimizer\n",
    "history2=0\n",
    "more_training=0\n",
    "if more_training==1:\n",
    "    compilemodel(model,keras.optimizers.RMSprop())\n",
    "    callback2 = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20)\n",
    "    history2=model.fit(\n",
    "        train_data, train_labels, epochs=epochs, batch_size=batch_size, \n",
    "        validation_data=(val_data, val_labels), \n",
    "        callbacks=callback2, verbose = 1\n",
    "        )\n",
    "    \n",
    "fitend = time.time()\n",
    "print(f'Time passed for fit: {fitend-fitstart:.1f} seconds')\n",
    "    \n",
    "# Save model and weights\n",
    "#create directory to save weighted model\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_model')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name + '.keras')\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path , '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting test and predictions for various scenarios\n",
    "\n",
    "#setting bigger font to ease the eyes\n",
    "font = {'family' : 'sans-serif',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "    # Estrazione dell'andamento dell'errore sul set di addestramento e sul set di validazione\n",
    "if more_training==1:\n",
    "    train_loss = history.history['loss'] + history2.history['loss']\n",
    "    val_loss = history.history['val_loss'] + history2.history['val_loss']\n",
    "else:\n",
    "    train_loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "# Creazione del grafico di errore di training e validazione\n",
    "plot_dir = os.path.join(os.getcwd(), 'model_plots')\n",
    "if not os.path.isdir(plot_dir):\n",
    "    os.makedirs(plot_dir)\n",
    "plot_name = model_name + '__errplot.png'\n",
    "plot_path = os.path.join(plot_dir, plot_name)\n",
    "    \n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.plot(train_loss, label='Perdita sul set di addestramento')\n",
    "plt.plot(val_loss, label='Perdita sul set di validazione')\n",
    "plt.xlabel('Epoche')\n",
    "plt.ylabel('Loss function')\n",
    "plt.ylim(0, 5)\n",
    "plt.grid(axis='both')\n",
    "plt.legend()\n",
    "#plt.title('Andamento dell\\'errore')\n",
    "plt.savefig(plot_path)\n",
    "\n",
    "    \n",
    "    #O_HH plot save\n",
    "input_file = 'saved_dataset/dataset_O_HH_100_5a_test.pickle' \n",
    "\n",
    "positions, energies = load_input_data(input_file)\n",
    "d1 = positions[:,0] #dH1>dH2\n",
    "d2 = positions[:,1]\n",
    "theta = positions[:,2]\n",
    "\n",
    "input_data = positions\n",
    "tf.function(autograph=False, reduce_retracing=True)\n",
    "    # Effettua previsioni sulle nuove osservazioni\n",
    "predictions = model.predict(input_data)\n",
    "\n",
    "    # Creazione del grafico di O_HH\n",
    "graph_dir = os.path.join(os.getcwd(), 'model_graphs_O_HH')\n",
    "if not os.path.isdir(graph_dir):\n",
    "    os.makedirs(graph_dir)\n",
    "graph1_name = model_name + '__O_HHfit.png'\n",
    "graph1_path = os.path.join(graph_dir, graph1_name)\n",
    "    \n",
    "fig1, ax1 = plt.subplots(figsize=(21.4,12))\n",
    "ax1.set(title='Predizione AI per O_HH: 0.1<theta<pi/2 e dH1=dH2= 2A')\n",
    "ax1.set(xlabel=\"Theta [rad]\", ylabel=\"Energy [eV]\")\n",
    "ax1.scatter(theta, energies, color ='salmon', label='pw.x energies')\n",
    "ax1.scatter(theta, predictions, color='darkcyan', label='AI predictions')\n",
    "ax1.grid(axis='both')\n",
    "ax1.legend(loc='best', ncol=1, facecolor= 'white')\n",
    "fig1.savefig(graph1_path, dpi = 200)\n",
    "\n",
    "    \n",
    "    #theta plot save\n",
    "input_file2 = 'saved_dataset/dataset_theta_100_5a_test.pickle' \n",
    "\n",
    "positions2, energies2 = load_input_data(input_file2)\n",
    "d12 = positions2[:,0] #dH1>dH2\n",
    "d22 = positions2[:,1]\n",
    "theta2 = positions2[:,2]\n",
    "\n",
    "input_data2 = positions2\n",
    "    # Effettua previsioni sulle nuove osservazioni\n",
    "predictions2 = model.predict(input_data2)\n",
    "\n",
    "    # Creazione del grafico di theta\n",
    "graph2_dir = os.path.join(os.getcwd(), 'model_graphs_theta')\n",
    "if not os.path.isdir(graph2_dir):\n",
    "    os.makedirs(graph2_dir)\n",
    "graph2_name = model_name + '__thetafit.png'\n",
    "graph2_path = os.path.join(graph2_dir, graph2_name)\n",
    "    \n",
    "fig2, ax2 = plt.subplots(figsize=(21.4,12))\n",
    "ax2.set(title='Predizione AI per 1<theta<pi e dH1=dH2= 0.96A')\n",
    "ax2.set(xlabel=\"Theta [rad]\", ylabel=\"Energy [eV]\")\n",
    "ax2.scatter(theta2, energies2, color ='salmon', label='pw.x energies')\n",
    "ax2.scatter(theta2, predictions2, color='darkcyan', label='AI predictions')\n",
    "ax2.grid(axis='both')\n",
    "ax2.legend(loc='best', ncol=1, facecolor= 'white')\n",
    "fig2.savefig(graph2_path, dpi = 200)\n",
    "\n",
    "    \n",
    "    #dh2 plot save\n",
    "input_file3 = 'saved_dataset/dataset_dh2_100_5a_test.pickle' \n",
    "\n",
    "positions3, energies3 = load_input_data(input_file3)\n",
    "d13 = positions3[:,0] #dH1>dH2\n",
    "d23 = positions3[:,1]\n",
    "theta3 = positions3[:,2]\n",
    "\n",
    "input_data3 = positions3\n",
    "    # Effettua previsioni sulle nuove osservazioni\n",
    "predictions3 = model.predict(input_data3)\n",
    "\n",
    "    # Creazione del grafico di theta\n",
    "graph3_dir = os.path.join(os.getcwd(), 'model_graphs_dh2')\n",
    "if not os.path.isdir(graph3_dir):\n",
    "    os.makedirs(graph3_dir)\n",
    "graph3_name = model_name + '__dh2fit.png'\n",
    "graph3_path = os.path.join(graph3_dir, graph3_name)\n",
    "    \n",
    "fig3, ax3 = plt.subplots(figsize=(21.4,12))\n",
    "ax3.set(title='Predizione AI per 0.6<dH2<3.6, dH1=0.96A, theta=1.82rad')\n",
    "ax3.set(xlabel=\"dH2 [Angstrom]\", ylabel=\"Energy [eV]\")\n",
    "ax3.scatter(d23, energies3, color ='salmon', label='pw.x energies')\n",
    "ax3.scatter(d23, predictions3, color='darkcyan', label='AI predictions')\n",
    "ax3.grid(axis='both')\n",
    "ax3.legend(loc='best', ncol=1, facecolor= 'white')\n",
    "fig3.savefig(graph3_path, dpi = 200)\n",
    "    \n",
    "    #OH_H plot save\n",
    "input_file4 = 'saved_dataset/dataset_OH_H_100_5a_test.pickle' \n",
    "\n",
    "positions4, energies4 = load_input_data(input_file4)\n",
    "d14 = positions4[:,0] #dH1>dH2\n",
    "d24 = positions4[:,1]\n",
    "theta4 = positions4[:,2]\n",
    "\n",
    "input_data4 = positions4\n",
    "    # Effettua previsioni sulle nuove osservazioni\n",
    "predictions4 = model.predict(input_data4)\n",
    "\n",
    "    # Creazione del grafico di OH_H\n",
    "graph4_dir = os.path.join(os.getcwd(), 'model_graphs_OH_H')\n",
    "if not os.path.isdir(graph4_dir):\n",
    "    os.makedirs(graph4_dir)\n",
    "graph4_name = model_name + '__OH_Hfit.png'\n",
    "graph4_path = os.path.join(graph4_dir, graph4_name)\n",
    "    \n",
    "fig4, ax4 = plt.subplots(figsize=(21.4,12))\n",
    "ax4.set(title='Predizione AI per OH_H: 0.6<dH1<2, theta=1.82rad e dH2= 3A')\n",
    "ax4.set(xlabel=\"dH1 [Angstrom]\", ylabel=\"Energy [eV]\")\n",
    "ax4.scatter(d14, energies4, color ='salmon', label='pw.x energies')\n",
    "ax4.scatter(d14, predictions4, color='darkcyan', label='AI predictions')\n",
    "ax4.grid(axis='both')\n",
    "ax4.legend(loc='best', ncol=1, facecolor= 'white')\n",
    "fig4.savefig(graph4_path, dpi = 200)  \n",
    "    \n",
    "        #OH_H plot save\n",
    "input_file5 = 'saved_dataset/dataset_O_H_H_100_5a_test.pickle' \n",
    "\n",
    "positions5, energies5 = load_input_data(input_file5)\n",
    "d15 = positions5[:,0] #dH1>dH2\n",
    "d25 = positions5[:,1]\n",
    "theta5 = positions5[:,2]\n",
    "\n",
    "input_data5 = positions5\n",
    "    # Effettua previsioni sulle nuove osservazioni\n",
    "predictions5 = model.predict(input_data5)\n",
    "\n",
    "    # Creazione del grafico di OH_H\n",
    "graph5_dir = os.path.join(os.getcwd(), 'model_graphs_O_H_H')\n",
    "if not os.path.isdir(graph5_dir):\n",
    "    os.makedirs(graph5_dir)\n",
    "graph5_name = model_name + '__O_H_Hfit.png'\n",
    "graph5_path = os.path.join(graph5_dir, graph5_name)\n",
    "    \n",
    "fig5, ax5 = plt.subplots(figsize=(21.4,12))\n",
    "ax5.set(title='Predizione AI per O_H_H: 0.5<dH1<3.5 A, theta=0.37 rad e dH2= 2A')\n",
    "ax5.set(xlabel=\"dH1 [Angstrom]\", ylabel=\"Energy [eV]\")\n",
    "ax5.scatter(d15, energies5, color ='salmon', label='pw.x energies')\n",
    "ax5.scatter(d15, predictions5, color='darkcyan', label='AI predictions')\n",
    "ax5.grid(axis='both')\n",
    "ax5.legend(loc='best', ncol=1, facecolor= 'white')\n",
    "fig5.savefig(graph5_path, dpi = 200)  \n",
    "    \n",
    "plt.close('all')\n",
    "    \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a94a04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
